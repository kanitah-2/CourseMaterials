{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Basic Probability and Statistics - Hypothesis Testing, Significance, P-Values\n",
    "<html>\n",
    "    <summary></summary>\n",
    "         <div> <p></p> </div>\n",
    "         <div style=\"font-size: 20px; width: 800px;\"> \n",
    "              <h1>\n",
    "               <left>Intro to Basic Probability and Statistics: Hypothesis Testing, Significance, P-Values.</left>\n",
    "              </h1>\n",
    "              <p><left>============================================================================</left> </p>\n",
    "<pre>Course: BIOM 421, Spring 2024\n",
    "Instructor: Brian Munsky\n",
    "Authors: Huy Vo, Ania Baetica, Brian Munsky\n",
    "Contact Info: munsky@colostate.edu\n",
    "</pre>\n",
    "         </div>\n",
    "    </p>\n",
    "\n",
    "</html>\n",
    "\n",
    "<details>\n",
    "  <summary>Copyright info</summary>\n",
    "\n",
    "```\n",
    "Copyright 2024 Brian Munsky\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "```\n",
    "<details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figuresC/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rng\n",
    "import scipy.stats as stats\n",
    "# !pip install statsmodels   # if you don't have it, uncomment this line\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figuresC/image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gs/29wgt2xj1jj873ytknj55rsr0000gp/T/ipykernel_58041/249685750.py:34: RuntimeWarning: invalid value encountered in log\n",
      "  LogNPairBefore = rng.lognormal(np.log(parametersPatients), 1)\n",
      "/var/folders/gs/29wgt2xj1jj873ytknj55rsr0000gp/T/ipykernel_58041/249685750.py:35: RuntimeWarning: invalid value encountered in log\n",
      "  LogNPairAfter = rng.lognormal(np.log(parametersPatients+effect), 1)\n"
     ]
    }
   ],
   "source": [
    "# Example Data Sets\n",
    "# In this notebook, we are going to use several different pairs of data sets to illustrate\n",
    "# different hypothesis tests.\n",
    "\n",
    "# The first two data sets are unpaired sets of random numbers drawn from a NORMAL distribution\n",
    "# with different means but the same standard deviations.\n",
    "Nnormal = [100,120]\n",
    "normal0 = rng.normal(0, 1, Nnormal[0])\n",
    "normal1 = rng.normal(0.5, 1, Nnormal[1])\n",
    "\n",
    "# The next pair are paired data sets, where each pair of numbers is drawn from a NORMAL distribution,\n",
    "# but the second number in each pair is drawn from a normal distribution with a mean that is\n",
    "# different than the mean of the first number in the pair. The standard deviation of the two\n",
    "# distributions is the same.\n",
    "Npaired = 100\n",
    "parametersPatients = rng.normal(10, 0.5, Npaired) # \n",
    "effect = 0.2\n",
    "NormalPairBefore = rng.normal(parametersPatients, 1) \n",
    "NormalPairAfter = rng.normal(parametersPatients+effect, 1) \n",
    "\n",
    "# The next pair of data sets are unpaired data sets drawn from a LOGNORMAL distribution with the\n",
    "# same standard deviation and the same means.\n",
    "Nlognormal = [100,120]\n",
    "lognormal0 = rng.lognormal(0, 1, Nlognormal[0])\n",
    "lognormal1 = rng.lognormal(0, 1, Nlognormal[1])\n",
    "\n",
    "# The next pair are paired data sets, where each pair of numbers is drawn from a LOGNORMAL distribution,\n",
    "# but the second number in each pair is drawn from a lognormal distribution with a mean that is\n",
    "# different than the mean of the first number in the pair. The standard deviation of the two\n",
    "# distributions is the same.\n",
    "Npaired = 100\n",
    "parametersPatients = rng.normal(0, 0.5, Npaired) \n",
    "effect = 0.2\n",
    "LogNPairBefore = rng.lognormal(np.log(parametersPatients), 1)\n",
    "LogNPairAfter = rng.lognormal(np.log(parametersPatients+effect), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "![alt text](figuresC/image-2.png)\n",
    "\n",
    "## The Testing Process\n",
    "\n",
    "![alt text](figuresC/image-3.png)\n",
    "\n",
    "## Interpreting Results\n",
    "\n",
    "The outcome of a hypothesis test provides insights into the plausibility of the null hypothesis. A rejection of the null hypothesis suggests evidence in favor of the alternative hypothesis, while a failure to reject implies insufficient evidence to warrant rejection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance and P-Values\n",
    "\n",
    "Hypothesis testing involves two critical concepts that serve as guiding beacons in our statistical journey: **significance** and **p-values**. These concepts provide us with quantitative measures to evaluate the strength of evidence in support of our hypotheses and to make informed decisions based on our analyses.\n",
    "\n",
    "## Significance: Separating Signal from Noise\n",
    "\n",
    "![alt text](figuresC/image-4.png)\n",
    "\n",
    "## Test Statistics: Quantifying Differences\n",
    "\n",
    "![alt text](figuresC/image-5.png)\n",
    "\n",
    "## P-Values: Assessing Evidence\n",
    "\n",
    "![alt text](figuresC/image-6.png)\n",
    "\n",
    "## Interpreting P-Values in the Context of Significance\n",
    "\n",
    "![alt text](figuresC/image-7.png)\n",
    "\n",
    "## Significance, P-Values, and Decision Making\n",
    "\n",
    "Significance, p-values, and test statistics play pivotal roles in the decision-making process during hypothesis testing. By setting a significance level ($\\alpha$) beforehand, we establish a threshold for rejecting the null hypothesis. If the calculated p-value falls below this threshold, and the test statistic exceeds critical values, we deem the results statistically significant and infer that the observed effect is unlikely to occur due to chance alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric and Non-Parametric Tests \n",
    "\n",
    "**Parametric tests** make assumptions about the distribution of the data, typically assuming that the data follow a specific probability distribution, such as the normal distribution (see t-test below.)\n",
    "\n",
    "**Non-parametric tests**, on the other hand, do not make assumptions about the distribution of the data or make fewer assumptions compared to parametric tests (see Wilcox signed rank test below). They are used when the data do not meet the assumptions of parametric tests or when the data are ordinal or non-normally distributed.\n",
    "\n",
    "Here are some key differences between parametric and non-parametric tests:\n",
    "\n",
    "**Assumptions**: Parametric tests make specific assumptions about the population distribution, such as normality and homogeneity of variances, while non-parametric tests make fewer or no assumptions about the distribution of the data.\n",
    "\n",
    "**Type of Data**: Parametric tests are often used for interval or ratio scale data that are normally distributed, while non-parametric tests are more flexible and can be used with ordinal, interval, or ratio scale data, as well as non-normally distributed data.\n",
    "\n",
    "**Test Statistics**: Parametric tests use test statistics that are based on the parameters of the population distribution (e.g., mean, standard deviation), while non-parametric tests use test statistics that are based on the ranks or order of the data.\n",
    "\n",
    "**Power**: Parametric tests tend to have higher statistical power (ability to detect true differences or effects) when the assumptions are met, while non-parametric tests may have lower power but are more robust to violations of assumptions.\n",
    "\n",
    "In summary, the choice between parametric and non-parametric tests depends on the nature of the data, the assumptions that can be made about the data, and the specific research question being addressed. If the assumptions of parametric tests are met, they are generally preferred because they can provide more powerful statistical tests. However, if the assumptions are violated, non-parametric tests may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Test Example -- Student's T-Test\n",
    "\n",
    "The **Student's t-test** is a **parametric statistical test** used to determine **is there is a significant difference between the means of two groups**. The t-test is commonly used in scientific research, especially in fields such as psychology, medicine, and biology, where it is used to compare means from experiments. For example, it often used to assess the effectiveness of interventions or treatments.\n",
    "\n",
    "It was developed by **William Sealy Gosset**, who published it **under the pseudonym \"Student\" in 1908** while working at the Guinness Brewery in Dublin, Ireland. \n",
    "\n",
    "The t-test is particularly useful when working with **small sample sizes** or **when the population standard deviation is unknown**.\n",
    "\n",
    "The formula for the t-test depends on whether the samples being compared are independent or dependent (paired). \n",
    "\n",
    "### Independent Samples t-test\n",
    "For independent samples, the t-statistic is computed by:\n",
    "$$\n",
    "t = \\frac{{\\bar{X}_1 - \\bar{X}_2}}{{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{align*}\n",
    "    & \\bar{X}_1 \\text{ and } \\bar{X}_2 \\text{ are the sample means of the two groups}, \\\\\n",
    "    & n_1 \\text{ and } n_2 \\text{ are the sample sizes of the two groups}, \\\\\n",
    "    & s_p \\text{ is the pooled standard deviation, given by:} \\\\\n",
    "    & s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}}, \\\\\n",
    "    & \\text{where } s_1 \\text{ and } s_2 \\text{ are the standard deviations of the two groups}.\n",
    "\\end{align*}\n",
    "\n",
    "Under the **Null Hypothesis** that the means of the two groups are equal, the t-statistic would follow a **t-distribution** with $n_1+n_2−2$ degrees of freedom.\n",
    "\n",
    "#### P-values\n",
    "Under the null hypothesis that the two distributions have the same mean, the t-statistic would be close to zero. The P-value using the t-test is found by calculating the probability that the a random sample drawn from from a t-distributon would be at least as large/small as the calculated t-statistic.  \n",
    "\n",
    "The probability that a larger $t$ would be sampled by chance is found by calculating **survival function** for the absolute value of $t$ (i.e., $\\text{surv}(|t|) = 1-CDF(|t|)$). \n",
    "\n",
    "*Two-Sided Test*: In a two-sided test, the null hypothesis typically states that there is no difference between the means of the two related groups, while the alternative hypothesis states that there is a difference, without specifying the direction of the difference. The two-sided test is used when you want to determine if there is a significant difference between the means in either direction. In this case, the critical region is split between the two tails of the distribution, and the p-value that at last as large a difference would be observed in the mean of the two populations is determined as $PV(t) = 2*\\text{surv}(|t|)$.\n",
    "\n",
    "*One-Sided Test*: In a one-sided test, the null hypothesis states that there is no difference or that group 2 is greater than or equal to group 1, while the alternative hypothesis specifies that group 1 is greater than group 2. The one-sided test is used when you are specifically interested in whether one group is greater than the other, but not if it's smaller. The critical region is located entirely in one tail of the distribution, and the p-value for testing if is determined as $PV(t) = \\text{surv}(t)$.\n",
    "\n",
    "#### Important limitations of the t-test:\n",
    "\n",
    "Like any test, the t-test has important limitations that one should consider before applying the test:\n",
    "\n",
    "1) **Assumption of Normality**: The t-test assumes that the data in each group are normally distributed. If this assumption is violated, the results of the test may be invalid.\n",
    "\n",
    "2) **Sensitive to Outliers**: The t-test can be sensitive to outliers, particularly when sample sizes are small.\n",
    "\n",
    "3) **Equal Variances Assumption**: The traditional independent samples t-test assumes that the variances of the two groups are equal. If this assumption is violated, alternative versions of the t-test, such as Welch's t-test, should be used.\n",
    "\n",
    "4) **Sample Size**: The t-test may not perform well with very small sample sizes, as the t-distribution approaches the normal distribution only as sample size increases.\n",
    "\n",
    "### Examples of One- and Two-Sided t-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test using our code: t = -4.151512028680217 p-value = 2.3693271918267008e-05\n",
      "t-test using built in code: t = -4.151512028680217 p-value = 2.3693271918267008e-05\n"
     ]
    }
   ],
   "source": [
    "# Illustration of the Independent Student's t-test\n",
    "\n",
    "# Define a function for the t-test\n",
    "def ttest(data0, data1, alternative='two-sided' ):\n",
    "    # Compute the number of samples in each group\n",
    "    N = [len(data0), len(data1)]\n",
    "    Ndof = N[0] + N[1] - 2 # number of degrees of freedom\n",
    "    \n",
    "    # Compute the t-statistic\n",
    "    sp = np.sqrt(((N[0]-1)*np.var(data0) + (N[1]-1)*np.var(data1)) / (N[0]+N[1]-2))\n",
    "    t = (np.mean(data0) - np.mean(data1)) / (sp * np.sqrt(1/N[0] + 1/N[1]))\n",
    "    \n",
    "    # Compute the p-value for 1- and 2-sided t-test.\n",
    "    if alternative=='two-sided':\n",
    "        pv = 2*stats.t.sf(np.abs(t), Ndof)\n",
    "    elif alternative=='greater':\n",
    "        pv = stats.t.sf(t, Ndof)\n",
    "    elif alternative=='less':\n",
    "        pv = stats.t.sf(-t, Ndof)\n",
    "   \n",
    "    return t, pv\n",
    "\n",
    "# Generate some data from two normal distributions with different means\n",
    "N = [10000,800] # number of samples in each group\n",
    "mu = [0.0,0.1] # true mean of each group\n",
    "sigma = 1.0 # true standard deviation (same for both groups)\n",
    "\n",
    "# Compute the t-test\n",
    "alternative = 'less'  # 'two-sided', 'greater', or 'less'\n",
    "t, pv = ttest(normal0, normal1, alternative=alternative)\n",
    "\n",
    "# Print the results of the t-test\n",
    "print('t-test using our code: t =', t, 'p-value =', pv)\n",
    "\n",
    "# Compare to built in t-test function:\n",
    "t_statistic, p_value = stats.ttest_ind(normal0, normal1, alternative=alternative)\n",
    "print('t-test using built in code: t =', t, 'p-value =', pv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Values t-test\n",
    "\n",
    "The **paired t-test** is a statistical test used to determine whether there is a significant difference between the means of two related groups. It's typically used when the samples being compared are not independent but instead have some sort of natural pairing or matching. For example, the paired t-test might be used to compare the effectiveness of a medication before and after treatment in the same group of patients.\n",
    "\n",
    "Here's how the paired t-test works:\n",
    "\n",
    "**Data Collection**: You collect data from two related groups, where each observation in one group is paired with a corresponding observation in the other group. For example, if you're comparing measurements before and after giving a hypothetical medication, each patient's measurement before treatment is paired with their measurement after treatment.\n",
    "\n",
    "**Calculate Differences**: For each pair of observations, calculate the difference between the two measurements. You'll end up with a new set of differences.\n",
    "\n",
    "**Calculate Sample Mean and Standard Deviation of Differences**: Calculate the mean ($d$) and standard deviation ($\\sigma_d$) of these differences.\n",
    "\n",
    "**Compute the t-statistic**: Use the formula for the one-sample t-test to calculate the t-statistic:\n",
    "\n",
    "$$ t=\\frac{d}{\\sigma_d/\\sqrt{n}} $$\n",
    "\n",
    "Where:\n",
    "* $d$ is the mean of the differences.\n",
    "* $\\sigma_d$​ is the standard deviation of the differences.\n",
    "* $n$ is the number of pairs.\n",
    "\n",
    "**Determine the p-value**: Use the t-distribution with $n-1$ degrees of freedom to determine the p-value associated with the calculated t-statistic.\n",
    "\n",
    "**Limitations**\n",
    "* The differences between pairs should be normally distributed.\n",
    "* The paired differences should be independent of each other.\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test using our code: t = -1.7654328473572931 p-value = 0.9604837808902538\n",
      "t-test using built in code: t = -1.7654328473572931 p-value = 0.9604837808902538\n"
     ]
    }
   ],
   "source": [
    "# Let's Illustrate of the Paired t-test\n",
    "# Generate some paired data\n",
    "def pairedTTest(data0,data1,alternative='two-sided'):\n",
    "    # Compute the number of samples in each group\n",
    "    N = len(data0)\n",
    "    Ndof = N - 1 # number of degrees of freedom\n",
    "    \n",
    "    # Compute the t-statistic\n",
    "    mu = np.mean(data0-data1)\n",
    "    sig = np.std(data0-data1)\n",
    "    t = mu / (sig / np.sqrt(N))\n",
    "    \n",
    "    # Compute the p-value for the 1- and 2-sided test:\n",
    "    if alternative=='two-sided':\n",
    "        pv = 2*stats.t.sf(np.abs(t), Ndof)\n",
    "    elif alternative=='greater':\n",
    "        pv = stats.t.sf(t, Ndof)\n",
    "    elif alternative=='less':\n",
    "        pv = stats.t.sf(-t, Ndof)\n",
    "    return t, pv    \n",
    "\n",
    "N = 100 # number of patients\n",
    "\n",
    "\n",
    "# Compute the t-test\n",
    "alternative = 'greater'  # 'two-sided', 'greater', or 'less'\n",
    "t, pv = ttest(NormalPairBefore, NormalPairAfter, alternative=alternative)\n",
    "\n",
    "# Print the results of the t-test\n",
    "print('t-test using our code: t =', t, 'p-value =', pv)\n",
    "\n",
    "# Compare to built in t-test function:\n",
    "t_statistic, p_value = stats.ttest_rel(NormalPairBefore, NormalPairAfter, alternative=alternative)\n",
    "print('t-test using built in code: t =', t, 'p-value =', pv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Test Example - Wilcox Ranked Sum Test (a.k.a, the Mann-Whitney U Test)\n",
    "\n",
    "This **Wilcox Ranked Sum** test is used to compare the **medians** of two independent groups. This **non-parametric test** is typically used when the assumptions of the independent samples (needed for the t-test above) are not met, such as when the data are not normally distributed or when the variances are not equal.\n",
    "\n",
    "Here's a brief overview of the Wilcoxon rank-sum test (Mann-Whitney U test):\n",
    "\n",
    "**Purpose**: The Wilcoxon rank-sum test is used to determine whether the distributions of two independent groups differ significantly in terms of their medians.\n",
    "\n",
    "**Assumptions**: The main assumption of the Wilcoxon rank-sum test is that the two groups are independent and that the data within each group are at least ordinal (that is there is some natural ranking for the data, i.g., $a<b<c<c<d,\\ldots $).\n",
    "\n",
    "**Null Hypotheses**.  Like the t-test, the Wilcox Ranked Sum can be evaluated as a **two-sided** or **one-sided** test.  The corresponding Null-Hypotheses are:     \n",
    "* **Two-Sided Test Null Hypothesis**:  The medians of the two independent groups are equal.\n",
    "* **Left Tailed Test Null Hypothesis**: The median of group 1 is greater than or equal to the median of group 2.\n",
    "* **Right Tailed Test Null Hypothesis**: The median of group 1 is less than or equal to the median of group 2.\n",
    "\n",
    "**Procedure**:\n",
    "1) Combine the data from the two groups into a single dataset.\n",
    "2) Rank all the observations in the combined dataset from smallest to largest, ignoring the group labels.\n",
    "3) Calculate the sum of ranks for each group.\n",
    "4) Use the smaller of the two sums of ranks as the test statistic (often denoted as U).\n",
    "5) Compare the test statistic to critical values from the Wilcoxon rank-sum distribution or calculate a p-value.\n",
    "\n",
    "**Interpretation**: If the test statistic is significantly different from what would be expected by chance, it suggests that the distributions of the two groups differ significantly in terms of their medians.\n",
    "\n",
    "**Limitations**\n",
    "The Wilcoxon rank-sum test is a non-parametric alternative to the independent samples t-test and is particularly useful when the assumptions of the t-test are not met. It does not assume normality or equal variances and is based on the ranks of the observations rather than their actual values.  However, like all tests, it has limitations:\n",
    "**Assumption of Independence**: The Wilcoxon rank-sum test assumes that the observations within each group are independent of each other and that the two groups are independent of each other. Violations of this assumption can lead to inaccurate results.\n",
    "\n",
    "**Ordinal Data Requirement**: While the Wilcoxon rank-sum test does not require the data to be normally distributed, it does require that the data be at least ordinal. If the data are purely nominal (e.g., categories without a natural order), the test may not be appropriate.\n",
    "\n",
    "**Sensitivity to Ties**: The Wilcoxon rank-sum test can be sensitive to ties (i.e., identical values) in the data, especially if there are many ties or if the ties are not evenly distributed between the groups. This can affect the accuracy of the test results.\n",
    "\n",
    "**Less Power Than the t-test**: In general, parametric tests such as the independent samples t-test tend to have more statistical power (i.e., are better at detecting true differences) than non-parametric tests like the Wilcoxon rank-sum test, especially when the assumptions of the parametric tests are met.\n",
    "\n",
    "**Difficulty in Interpreting Effect Size**: While the Wilcoxon rank-sum test can determine whether there is a statistically significant difference between the groups, it does not provide a direct measure of effect size. Interpreting the practical significance of the results can therefore be more challenging compared to parametric tests.\n",
    "\n",
    "## Example for Performing the Wilcox Ranked Sum test for unpaired data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon ranked sum test: t = -4.150124568811349 p-value = 3.3229438283827576e-05\n"
     ]
    }
   ],
   "source": [
    "# Now, let's apply the Wilcoxon Ranked Sum Test to our original data sets\n",
    "# Compute the Wilcoxon ranked sum test\n",
    "t, pv = stats.ranksums(normal0, normal1)\n",
    "print('Wilcoxon ranked sum test: t =', t, 'p-value =', pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Performing the Wilcox Ranked Sum test for unpaired data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon ranked sum test: t = 2026.0 p-value = 0.08621255219588186\n",
      "t-test: t = -1.9755324286261098 p-value = 0.05099170915484463\n"
     ]
    }
   ],
   "source": [
    "# Now, let's apply the Wilcoxon Ranked Sum Test to our paired data sets\n",
    "# Compute the Wilcoxon ranked sum test\n",
    "t, pv = stats.wilcoxon(NormalPairBefore, NormalPairAfter)\n",
    "print('Wilcoxon ranked sum test: t =', t, 'p-value =', pv)\n",
    "\n",
    "# Let's see how that compares to the t-test\n",
    "t, pv = pairedTTest(NormalPairBefore, NormalPairAfter)\n",
    "print('t-test: t =', t, 'p-value =', pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA (ANalysis Of VAriance)\n",
    "\n",
    "![alt text](figuresC/image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Performing the ANOVA\n",
    "![alt text](figuresC/image-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of the ANOVA test in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our own (Unoptimized)) Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA (Custom): F = 19.85865854393886 p-value = 8.042907007066447e-09\n"
     ]
    }
   ],
   "source": [
    "# example of a data set on which to perform ANOVA\n",
    "N = 100\n",
    "Ngroups = 3\n",
    "group0 = rng.normal(0, 1, N)\n",
    "group1 = rng.normal(0.01, 1, N)\n",
    "group2 = rng.normal(1, 1, N)\n",
    "\n",
    "# Running the ANOVA test without the use of the built-in function\n",
    "# compute the f statistic\n",
    "N = [len(group0), len(group1), len(group2)]\n",
    "Ngroups = len(N)\n",
    "Ndata = N[0] + N[1] + N[2]\n",
    "data = np.concatenate((group0, group1, group2))\n",
    "\n",
    "# Compute the group means and overall means.\n",
    "grandMean = np.mean(data)\n",
    "groupMean = [np.mean(group0), np.mean(group1), np.mean(group2)]\n",
    "\n",
    "# Compute the group variances\n",
    "groupVar = [np.var(group0), np.var(group1), np.var(group2)]\n",
    "\n",
    "# Compute the within group mean square differences\n",
    "dfwithin = Ndata - Ngroups\n",
    "SSW = np.dot(N, groupVar)\n",
    "MSwithin = SSW / dfwithin\n",
    "\n",
    "# Compute the between group mean square differences\n",
    "dfbetween = Ngroups - 1\n",
    "SSB = np.dot(N, (groupMean - grandMean)**2)\n",
    "MSbetween = SSB / dfbetween\n",
    "\n",
    "# Comptue teh F-statistic\n",
    "F = MSbetween / MSwithin\n",
    "\n",
    "# Compute the p-value using the F-distribution\n",
    "pv = 1 - stats.f.cdf(F, dfbetween, dfwithin)\n",
    "print('ANOVA (Custom): F =', F, 'p-value =', pv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Builtin Stats routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA (Built in): F = 19.858658543938862 p-value = 8.042907016721898e-09\n"
     ]
    }
   ],
   "source": [
    "# Now, lets compare our result to the built-in ANOVA function\n",
    "F, pv = stats.f_oneway(group0, group1, group2)\n",
    "print('ANOVA (Built in): F =', F, 'p-value =', pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tukey HSD Test\n",
    "\n",
    "Upon rejecting the null hypothesis in the ANOVA test, we have evidence that the groups do not all have the same mean, but we do not yet know which groups are significantly different from which other groups. And, when there are lots of groups (e.g., lots of different drugs being tested), this may not be a trivial concern.\n",
    "\n",
    "The Tukey HSD test is a pairwise comparison test, and it is used to determine which groups are different from which other groups. The test is based on the studentized range distribution, and it is used to determine the minimum difference between group means that is statistically significant.  The more groups that there are the more likely it is that some pairs will be different by mere chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "==================================================\n",
      "group1 group2 meandiff p-adj  lower  upper  reject\n",
      "--------------------------------------------------\n",
      "     0      1   0.7071    0.0 0.3799 1.0344   True\n",
      "     0      2   1.1426    0.0 0.8153 1.4698   True\n",
      "     1      2   0.4354 0.0054 0.1082 0.7627   True\n",
      "--------------------------------------------------\n",
      "[1.90537314e-06 0.00000000e+00 5.37179390e-03]\n"
     ]
    }
   ],
   "source": [
    "# Illustration of the Tukey Honestly Significant Difference (HSD) Test\n",
    "\n",
    "#mLet's generate some data to use for the Tukey HSD test\n",
    "N = 100\n",
    "Ngroups = 3\n",
    "means = np.zeros(Ngroups)\n",
    "stds = np.ones(Ngroups)\n",
    "\n",
    "# Change the means for the last two groups\n",
    "means[-2:] = [0.5,1.0]\n",
    "\n",
    "groups = [rng.normal(means[i], stds[i], N) for i in range(Ngroups)]\n",
    "\n",
    "from statsmodels.stats.multicomp import MultiComparison \n",
    "from statsmodels.stats.libqsturng import qsturng\n",
    "\n",
    "# Create the data\n",
    "data = np.concatenate(groups)\n",
    "groupLabels = np.concatenate([[i]*N for i in range(Ngroups)])\n",
    "\n",
    "# Create a MultiComparison object\n",
    "mc = MultiComparison(data, groupLabels)\n",
    "\n",
    "# Compute the Tukey HSD test\n",
    "result = mc.tukeyhsd()\n",
    "print(result)\n",
    "# Print the p-values\n",
    "print(result.pvalues)\n",
    "\n",
    "# Now go back and change the number of groups and see how the p-values change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
